{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%` not found.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import Markdown, display\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "\n",
    "def printmd(string):\n",
    "    display(Markdown(string))\n",
    "    \n",
    "    \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "\n",
    "%  matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adult = pd.read_csv('adult.csv')\n",
    "\n",
    "column_names = ['age', 'workclass', 'fnlwgt', 'education', 'educational-num','marital-status', 'occupation', 'relationship', 'race', 'gender','capital-gain', 'capital-loss', 'hours-per-week', 'native-country','income']\n",
    "\n",
    "train = pd.read_csv('adult.data', sep=\",\\s\", header=None, names = column_names, engine = 'python')\n",
    "test = pd.read_csv('adult.test', sep=\",\\s\", header=None, names = column_names, engine = 'python')\n",
    "test['income'].replace(regex=True,inplace=True,to_replace=r'\\.',value=r'')\n",
    "\n",
    "\n",
    "adult = pd.concat([test,train])\n",
    "adult.reset_index(inplace = True, drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 1. Preliminary Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## 1.1. Columns and their types"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 48843 entries, 0 to 48842\n",
      "Data columns (total 15 columns):\n",
      " #   Column           Non-Null Count  Dtype   \n",
      "---  ------           --------------  -----   \n",
      " 0   age              48843 non-null  category\n",
      " 1   workclass        48842 non-null  category\n",
      " 2   fnlwgt           48842 non-null  float64 \n",
      " 3   education        48842 non-null  category\n",
      " 4   educational-num  48842 non-null  float64 \n",
      " 5   marital-status   48842 non-null  category\n",
      " 6   occupation       48842 non-null  category\n",
      " 7   relationship     48842 non-null  category\n",
      " 8   race             48842 non-null  category\n",
      " 9   gender           48842 non-null  category\n",
      " 10  capital-gain     48842 non-null  float64 \n",
      " 11  capital-loss     48842 non-null  float64 \n",
      " 12  hours-per-week   48842 non-null  float64 \n",
      " 13  native-country   48842 non-null  category\n",
      " 14  income           48842 non-null  category\n",
      "dtypes: category(10), float64(5)\n",
      "memory usage: 2.4 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Setting all the categorical columns to type category\n",
    "for col in set(adult.columns) - set(adult.describe().columns):\n",
    "    adult[col] = adult[col].astype('category')\n",
    "    \n",
    "printmd('## 1.1. Columns and their types')\n",
    "print(adult.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## 1.2. Data"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>educational-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>gender</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>|1x3 Cross validator</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25</td>\n",
       "      <td>Private</td>\n",
       "      <td>226802.0</td>\n",
       "      <td>11th</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>89814.0</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Farming-fishing</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28</td>\n",
       "      <td>Local-gov</td>\n",
       "      <td>336951.0</td>\n",
       "      <td>Assoc-acdm</td>\n",
       "      <td>12.0</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Protective-serv</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>44</td>\n",
       "      <td>Private</td>\n",
       "      <td>160323.0</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>7688.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    age  workclass    fnlwgt     education  educational-num  \\\n",
       "0  |1x3 Cross validator        NaN       NaN           NaN              NaN   \n",
       "1                    25    Private  226802.0          11th              7.0   \n",
       "2                    38    Private   89814.0       HS-grad              9.0   \n",
       "3                    28  Local-gov  336951.0    Assoc-acdm             12.0   \n",
       "4                    44    Private  160323.0  Some-college             10.0   \n",
       "\n",
       "       marital-status         occupation relationship   race gender  \\\n",
       "0                 NaN                NaN          NaN    NaN    NaN   \n",
       "1       Never-married  Machine-op-inspct    Own-child  Black   Male   \n",
       "2  Married-civ-spouse    Farming-fishing      Husband  White   Male   \n",
       "3  Married-civ-spouse    Protective-serv      Husband  White   Male   \n",
       "4  Married-civ-spouse  Machine-op-inspct      Husband  Black   Male   \n",
       "\n",
       "   capital-gain  capital-loss  hours-per-week native-country income  \n",
       "0           NaN           NaN             NaN            NaN    NaN  \n",
       "1           0.0           0.0            40.0  United-States  <=50K  \n",
       "2           0.0           0.0            50.0  United-States  <=50K  \n",
       "3           0.0           0.0            40.0  United-States   >50K  \n",
       "4        7688.0           0.0            40.0  United-States   >50K  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Top 5 records\n",
    "printmd('## 1.2. Data')\n",
    "adult.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## 1.3. Summary Statistics"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>educational-num</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4.884200e+04</td>\n",
       "      <td>48842.000000</td>\n",
       "      <td>48842.000000</td>\n",
       "      <td>48842.000000</td>\n",
       "      <td>48842.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.896641e+05</td>\n",
       "      <td>10.078089</td>\n",
       "      <td>1079.067626</td>\n",
       "      <td>87.502314</td>\n",
       "      <td>40.422382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.056040e+05</td>\n",
       "      <td>2.570973</td>\n",
       "      <td>7452.019058</td>\n",
       "      <td>403.004552</td>\n",
       "      <td>12.391444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.228500e+04</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.175505e+05</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.781445e+05</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.376420e+05</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>45.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.490400e+06</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>99999.000000</td>\n",
       "      <td>4356.000000</td>\n",
       "      <td>99.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             fnlwgt  educational-num  capital-gain  capital-loss  \\\n",
       "count  4.884200e+04     48842.000000  48842.000000  48842.000000   \n",
       "mean   1.896641e+05        10.078089   1079.067626     87.502314   \n",
       "std    1.056040e+05         2.570973   7452.019058    403.004552   \n",
       "min    1.228500e+04         1.000000      0.000000      0.000000   \n",
       "25%    1.175505e+05         9.000000      0.000000      0.000000   \n",
       "50%    1.781445e+05        10.000000      0.000000      0.000000   \n",
       "75%    2.376420e+05        12.000000      0.000000      0.000000   \n",
       "max    1.490400e+06        16.000000  99999.000000   4356.000000   \n",
       "\n",
       "       hours-per-week  \n",
       "count    48842.000000  \n",
       "mean        40.422382  \n",
       "std         12.391444  \n",
       "min          1.000000  \n",
       "25%         40.000000  \n",
       "50%         40.000000  \n",
       "75%         45.000000  \n",
       "max         99.000000  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "printmd('## 1.3. Summary Statistics')\n",
    "\n",
    "adult.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## 1.4. Missing values"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "workclass: 2799 records"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "occupation: 2809 records"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "native-country: 857 records"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "printmd('## 1.4. Missing values')\n",
    "for i,j in zip(adult.columns,(adult.values.astype(str) == '?').sum(axis = 0)):\n",
    "    if j > 0:\n",
    "        printmd(str(i) + ': ' + str(j) + ' records')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treating Missing Values by predicting them\n",
    "\n",
    "I fill the missing values in each of the three columns by predicting their values. For each of the three columns, I use all the attributes (including 'income') as independent variables and treat that column as the dependent variable, making it a multi-class classification task. I use three classification algorithms, namely, logistic regression, decision trees and random forest to predict the class when the value is missing (in this case a '?'). I then take a majority vote amongst the three classifiers to be the class of the missing value. In case of a tie, I pick the majority class of that column using the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create one hot encoding of the categorical columns in the data frame.\n",
    "def oneHotCatVars(df, df_cols):\n",
    "    \n",
    "    df_1 = adult_data = df.drop(columns = df_cols, axis = 1)\n",
    "    df_2 = pd.get_dummies(df[df_cols])\n",
    "    \n",
    "    return (pd.concat([df_1, df_2], axis=1, join='inner'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### 1.4.1. Filling in missing values for Attribute workclass"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nLogisticRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23984/1138532515.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mlog_reg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mlog_reg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_label\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[0mlog_reg_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlog_reg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1194\u001b[0m             \u001b[0m_dtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1196\u001b[1;33m         X, y = self._validate_data(\n\u001b[0m\u001b[0;32m   1197\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1198\u001b[0m             \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    582\u001b[0m                 \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"y\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    583\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 584\u001b[1;33m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    585\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    586\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1104\u001b[0m         )\n\u001b[0;32m   1105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1106\u001b[1;33m     X = check_array(\n\u001b[0m\u001b[0;32m   1107\u001b[0m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1108\u001b[0m         \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maccept_sparse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    919\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    920\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 921\u001b[1;33m             _assert_all_finite(\n\u001b[0m\u001b[0;32m    922\u001b[0m                 \u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    923\u001b[0m                 \u001b[0minput_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    159\u001b[0m                 \u001b[1;34m\"#estimators-that-handle-nan-values\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m             )\n\u001b[1;32m--> 161\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg_err\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input X contains NaN.\nLogisticRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "printmd('### 1.4.1. Filling in missing values for Attribute workclass')\n",
    "\n",
    "test_data = adult[(adult.workclass.values == '?')].copy()\n",
    "test_label = test_data.workclass\n",
    "\n",
    "train_data = adult[(adult.workclass.values != '?')].copy()\n",
    "train_label = train_data.workclass\n",
    "\n",
    "test_data.drop(columns = ['workclass'], inplace = True)\n",
    "train_data.drop(columns = ['workclass'], inplace = True)\n",
    "\n",
    "train_data = oneHotCatVars(train_data, train_data.select_dtypes('category').columns)\n",
    "test_data = oneHotCatVars(test_data, test_data.select_dtypes('category').columns)\n",
    "\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(train_data, train_label)\n",
    "log_reg_pred = log_reg.predict(test_data)\n",
    "\n",
    "\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(train_data, train_label)\n",
    "clf_pred = clf.predict(test_data)\n",
    "\n",
    "r_forest = RandomForestClassifier(n_estimators=10)\n",
    "r_forest.fit(train_data, train_label)\n",
    "r_forest_pred = r_forest.predict(test_data)\n",
    "\n",
    "majority_class = adult.workclass.value_counts().index[0]\n",
    "\n",
    "pred_df =  pd.DataFrame({'RFor': r_forest_pred, 'DTree' : clf_pred, 'LogReg' : log_reg_pred})\n",
    "overall_pred = pred_df.apply(lambda x: x.value_counts().index[0] if x.value_counts()[0] > 1 else majority_class, axis = 1)\n",
    "\n",
    "adult.loc[(adult.workclass.values == '?'),'workclass'] = overall_pred.values\n",
    "print(adult.workclass.value_counts())\n",
    "print(adult.workclass.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printmd('### 1.4.2. Filling in missing values for Occupation occupation')\n",
    "\n",
    "test_data = adult[(adult.occupation.values == '?')].copy()\n",
    "test_label = test_data.occupation\n",
    "\n",
    "train_data = adult[(adult.occupation.values != '?')].copy()\n",
    "train_label = train_data.occupation\n",
    "\n",
    "test_data.drop(columns = ['occupation'], inplace = True)\n",
    "train_data.drop(columns = ['occupation'], inplace = True)\n",
    "\n",
    "train_data = oneHotCatVars(train_data, train_data.select_dtypes('category').columns)\n",
    "test_data = oneHotCatVars(test_data, test_data.select_dtypes('category').columns)\n",
    "\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(train_data, train_label)\n",
    "log_reg_pred = log_reg.predict(test_data)\n",
    "\n",
    "\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(train_data, train_label)\n",
    "clf_pred = clf.predict(test_data)\n",
    "\n",
    "r_forest = RandomForestClassifier(n_estimators=10)\n",
    "r_forest.fit(train_data, train_label)\n",
    "r_forest_pred = r_forest.predict(test_data)\n",
    "\n",
    "\n",
    "majority_class = adult.occupation.value_counts().index[0]\n",
    "\n",
    "pred_df =  pd.DataFrame({'RFor': r_forest_pred, 'DTree' : clf_pred, 'LogReg' : log_reg_pred})\n",
    "overall_pred = pred_df.apply(lambda x: x.value_counts().index[0] if x.value_counts()[0] > 1 else majority_class, axis = 1)\n",
    "\n",
    "adult.loc[(adult.occupation.values == '?'),'occupation'] = overall_pred.values\n",
    "print(adult.occupation.value_counts())\n",
    "print(adult.occupation.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printmd('### 1.4.3. Filling in missing values for Native Country')\n",
    "\n",
    "test_data = adult[(adult['native-country'].values == '?')].copy()\n",
    "test_label = test_data['native-country']\n",
    "\n",
    "train_data = adult[(adult['native-country'].values != '?')].copy()\n",
    "train_label = train_data['native-country']\n",
    "\n",
    "test_data.drop(columns = ['native-country'], inplace = True)\n",
    "train_data.drop(columns = ['native-country'], inplace = True)\n",
    "\n",
    "train_data = oneHotCatVars(train_data, train_data.select_dtypes('category').columns)\n",
    "test_data = oneHotCatVars(test_data, test_data.select_dtypes('category').columns)\n",
    "\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(train_data, train_label)\n",
    "log_reg_pred = log_reg.predict(test_data)\n",
    "\n",
    "\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(train_data, train_label)\n",
    "clf_pred = clf.predict(test_data)\n",
    "\n",
    "r_forest = RandomForestClassifier(n_estimators=10)\n",
    "r_forest.fit(train_data, train_label)\n",
    "r_forest_pred = r_forest.predict(test_data)\n",
    "\n",
    "\n",
    "majority_class = adult['native-country'].value_counts().index[0]\n",
    "\n",
    "pred_df =  pd.DataFrame({'RFor': r_forest_pred, 'DTree' : clf_pred, 'LogReg' : log_reg_pred})\n",
    "overall_pred = pred_df.apply(lambda x: x.value_counts().index[0] if x.value_counts()[0] > 1 else majority_class, axis = 1)\n",
    "\n",
    "adult.loc[(adult['native-country'].values == '?'),'native-country'] = overall_pred.values\n",
    "print(adult['native-country'].value_counts())\n",
    "print(adult['native-country'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resetting the categories\n",
    "\n",
    "adult['workclass'] = adult['workclass'].cat.remove_categories('?')\n",
    "adult['occupation'] = adult['occupation'].cat.remove_categories('?')\n",
    "adult['native-country'] = adult['native-country'].cat.remove_categories('?')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printmd('## 1.5. Correlation Matrix')\n",
    "\n",
    "display(adult.corr())\n",
    "\n",
    "printmd('We see that none of the columns are highly correlated.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. Exploratory Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dictionary that contain the education and it's corresponding education level\n",
    "edu_level = {}\n",
    "for x,y in adult[['educational-num','education']].drop_duplicates().itertuples(index=False):\n",
    "    edu_level[y] = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printmd('## 2.1. Education vs Income')\n",
    "\n",
    "education = round(pd.crosstab(adult.education, adult.income).div(pd.crosstab(adult.education, adult.income).apply(sum,1),0),2)\n",
    "education = education.reindex(sorted(edu_level, key=edu_level.get, reverse=False))\n",
    "\n",
    "ax = education.plot(kind ='bar', title = 'Proportion distribution across education levels', figsize = (10,8))\n",
    "ax.set_xlabel('Education level')\n",
    "ax.set_ylabel('Proportion of population')\n",
    "\n",
    "printmd('I plot a bar graph showing the proportion of income classes across education levels \\\n",
    "        in the figure below. As one would expect, we see from the bar graph below that as the \\\n",
    "        education level increase, the proportion of people who earn more than 50k a year also \\\n",
    "        increase. It is interesting to note that only after a master\\'s degree, the proportion of \\\n",
    "        people earning more than 50k a year, is a majority.')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printmd('## 2.2 Gender vs Income')\n",
    "\n",
    "gender = round(pd.crosstab(adult.gender, adult.income).div(pd.crosstab(adult.gender, adult.income).apply(sum,1),0),2)\n",
    "gender.sort_values(by = '>50K', inplace = True)\n",
    "ax = gender.plot(kind ='bar', title = 'Proportion distribution across gender levels')\n",
    "ax.set_xlabel('Gender level')\n",
    "ax.set_ylabel('Proportion of population')\n",
    "\n",
    "printmd('We plot a bar graph showing the proportion of income classes across the genders in figure \\\n",
    "        below. From the graph, at an overall view, there exists a wage gap between females and males. \\\n",
    "        Since we do not have the exactly value of the income, we are limited to only observing that the \\\n",
    "        proportion of males earning more than 50k a year is more than double of their female counterparts. \\\n",
    "        ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gender_workclass = round(pd.crosstab(adult.workclass, [adult.income, adult.gender]).div(pd.crosstab(adult.workclass, [adult.income, adult.gender]).apply(sum,1),0),2)\n",
    "gender_workclass[[('>50K','Male'), ('>50K','Female')]].plot(kind = 'bar', title = 'Proportion distribution across gender for each workclass', figsize = (10,8), rot = 30)\n",
    "ax.set_xlabel('Gender level')\n",
    "ax.set_ylabel('Proportion of population')\n",
    "\n",
    "printmd('Taking a closer look at the disparity in income between men and women, plot the \\\n",
    "        proportion of men and women who earn more than 50k a year, across all the working \\\n",
    "        classes as seen in Fig. 3. We see that men always have a higher proportion earning \\\n",
    "        more than 50k a year than women, except for the \\'without.pay\\' working class, where \\\n",
    "        they have the same proportion.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# printmd('### Gender across working classes')\n",
    "\n",
    "# for i in adult.workclass.unique():\n",
    "#     df = adult[adult.workclass == i]\n",
    "\n",
    "#     hours_per_week = round(pd.crosstab(df.gender, df.income).div(pd.crosstab(df.gender, df.income).apply(sum,1),0),2)\n",
    "#     # hours_per_week.sort_values(by = '>50K', inplace = True)\n",
    "#     ax = hours_per_week.plot(kind ='bar', title = 'Proportion distribution across Gender for '+ i)\n",
    "#     ax.set_xlabel('Gender')\n",
    "#     ax.set_ylabel('Proportion of population')\n",
    "\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printmd('## 2.3. Occupation vs Income')\n",
    "\n",
    "occupation = round(pd.crosstab(adult.occupation, adult.income).div(pd.crosstab(adult.occupation, adult.income).apply(sum,1),0),2)\n",
    "occupation.sort_values(by = '>50K', inplace = True)\n",
    "ax = occupation.plot(kind ='bar', title = 'Proportion distribution across Occupation levels', figsize = (10,8))\n",
    "ax.set_xlabel('Occupation level')\n",
    "ax.set_ylabel('Proportion of population')\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printmd('## 2.4. Workclass vs Income')\n",
    "\n",
    "workclass = round(pd.crosstab(adult.workclass, adult.income).div(pd.crosstab(adult.workclass, adult.income).apply(sum,1),0),2)\n",
    "workclass.sort_values(by = '>50K', inplace = True)\n",
    "ax = workclass.plot(kind ='bar', title = 'Proportion distribution across workclass levels', figsize = (10,8))\n",
    "ax.set_xlabel('Workclass level')\n",
    "ax.set_ylabel('Proportion of population')\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printmd('## 2.5. Race vs Income')\n",
    "\n",
    "race = round(pd.crosstab(adult.race, adult.income).div(pd.crosstab(adult.race, adult.income).apply(sum,1),0),2)\n",
    "race.sort_values(by = '>50K', inplace = True)\n",
    "ax = race.plot(kind ='bar', title = 'Proportion distribution across race levels', figsize = (10,8))\n",
    "ax.set_xlabel('Race level')\n",
    "ax.set_ylabel('Proportion of population')\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printmd('## 2.6. Native Country')\n",
    "\n",
    "native_country = round(pd.crosstab(adult['native-country'], adult.income).div(pd.crosstab(adult['native-country'], adult.income).apply(sum,1),0),2)\n",
    "native_country.sort_values(by = '>50K', inplace = True)\n",
    "ax = native_country.plot(kind ='bar', title = 'Proportion distribution across Native Country levels', figsize = (20,12))\n",
    "ax.set_xlabel('Native country')\n",
    "ax.set_ylabel('Proportion of population')\n",
    "\n",
    "printmd('I plot a bar graph showing the proportion of income classes across the native country in figure \\\n",
    "        below. From the graph, we notice a trend in positioning of the country. South American country are \\\n",
    "        at the left end of the plot, with low proportion of population that make more than 50k a year. The \\\n",
    "        United States is located somewhat centrally, and at the right are countries from Europe and Asia, \\\n",
    "        with higher proportion of population that make more than 50k a year.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printmd('## 2.7. Hours per week vs Income')\n",
    "\n",
    "hours_per_week = round(pd.crosstab(adult['hours-per-week'], adult.income).div(pd.crosstab(adult['hours-per-week'], adult.income).apply(sum,1),0),2)\n",
    "# hours_per_week.sort_values(by = '>50K', inplace = True)\n",
    "ax = hours_per_week.plot(kind ='bar', title = 'Proportion distribution across Hours per week', figsize = (20,12))\n",
    "ax.set_xlabel('Hours per week')\n",
    "ax.set_ylabel('Proportion of population')\n",
    "\n",
    "printmd('I plot a bar graph showing the proportion of income classes across the hours worked. \\\n",
    "        We would expected to notice a trend that higher the hours worked per week, the higher the proportion \\\n",
    "        of population making more than 50k a year. However, this was not necessarily true from the graph. \\\n",
    "        For several hours instance (for example, where hours worked was 77, 79, 81, 82, 87, 88 and so on) \\\n",
    "        no one earned more than 50k a year. ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printmd('### 2.7.1 Hours per week with categories')\n",
    "\n",
    "adult['hour_worked_bins'] = ['<40' if i < 40 else '40-60' if i <= 60 else '>60'  for i in adult['hours-per-week']]\n",
    "adult['hour_worked_bins'] = adult['hour_worked_bins'].astype('category')\n",
    "hours_per_week = round(pd.crosstab(adult.hour_worked_bins, adult.income).div(pd.crosstab(adult.hour_worked_bins, adult.income).apply(sum,1),0),2)\n",
    "\n",
    "hours_per_week.sort_values(by = '>50K', inplace = True)\n",
    "ax = hours_per_week.plot(kind ='bar', title = 'Proportion distribution across Hours per week', figsize = (10,6))\n",
    "ax.set_xlabel('Hours per week')\n",
    "ax.set_ylabel('Proportion of population')\n",
    "\n",
    "printmd('Therefore, I decided to transform this column into 3 categories, less than 40 hours, \\\n",
    "        40 to 60 hours and greater than 60 hours. Plotting a bar graph with these 3 categories, \\\n",
    "        we can see from the figure below that there is an increasing trend in the proportion of \\\n",
    "        population making more than 50k a year.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "printmd('### 2.7.2 Hours worked across working classes')\n",
    "\n",
    "for i in adult.workclass.unique():\n",
    "    df = adult[adult.workclass == i]\n",
    "\n",
    "    hours_per_week = round(pd.crosstab(df['hours-per-week'], df.income).div(pd.crosstab(df['hours-per-week'], df.income).apply(sum,1),0),2)\n",
    "    # hours_per_week.sort_values(by = '>50K', inplace = True)\n",
    "    ax = hours_per_week.plot(kind ='bar', title = 'Proportion distribution across Hours per week for '+ i, figsize = (20,12))\n",
    "    ax.set_xlabel('Hours per week')\n",
    "    ax.set_ylabel('Proportion of population')\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.rcParams['figure.figsize'] = (5, 4)\n",
    "# #plt.xlim([20,110])\n",
    "# #plt.ylim([20,110])\n",
    "            \n",
    "# pos = adult[adult.income == '>50K']\n",
    "# neg = adult[adult.income != '>50K']\n",
    "\n",
    "\n",
    "# plt.scatter(pos['educational-num'], pos['hours-per-week'], marker='o', c='b')\n",
    "# plt.scatter(neg['educational-num'], neg['hours-per-week'], marker='x', c='r')\n",
    "# plt.xlabel('Exam 1 score')\n",
    "# plt.ylabel('Exam 2 score')\n",
    "# plt.legend(['Not Admitted', 'Admitted'])\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 3. Data Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove education and fnlwgt\n",
    "#adult.drop(columns = ['education','fnlwgt','hours-per-week'], inplace = True)\n",
    "\n",
    "printmd('* For education level, we have 2 features that convey the same meaning, \\'education\\' \\\n",
    "        and \\'educational-num\\'. To avoid the effect of this attribute on the models to be \\\n",
    "        overstated, I am not going to use the categorical education attribute.')\n",
    "printmd('* I use the categorical Hours work column and drop the \\'hour-per-week\\' column')\n",
    "printmd('* Also, I chose not to use the \\'Fnlwgt\\' attribute that is used by the census, \\\n",
    "        as the inverse of sampling fraction adjusted for non-response and over or under sampling \\\n",
    "        of particular groups. This attribute does not convey individual related meaning.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printmd('## Box plot')\n",
    "adult.select_dtypes(exclude = 'category').plot(kind = 'box', figsize = (10,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printmd ('Normalization happens on the training dataset, by removing the mean and \\\n",
    "        scaling to unit variance. These values are stored and then later applied  \\\n",
    "        to the test data before the test data is passed to the model for prediction. ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 4. Model Development & Classification "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Data Preparation'\n",
    "\n",
    "One-hot encoding is the process of representing multi-class categorical features as binary features, one for each class. Although this process increases the dimensionality of the dataset, classification algorithms tend to work better on this format of data.\n",
    "\n",
    "I use one-hot encoding to represent all the categorical features in the dataset. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Prep\n",
    "adult_data = adult.drop(columns = ['income'])\n",
    "adult_label = adult.income\n",
    "\n",
    "\n",
    "adult_cat_1hot = pd.get_dummies(adult_data.select_dtypes('category'))\n",
    "adult_non_cat = adult_data.select_dtypes(exclude = 'category')\n",
    "\n",
    "adult_data_1hot = pd.concat([adult_non_cat, adult_cat_1hot], axis=1, join='inner')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train - Test split\n",
    "train_data, test_data, train_label, test_label = train_test_split(adult_data_1hot, adult_label, test_size  = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization\n",
    "from sklearn.preprocessing import StandardScaler  \n",
    "scaler = StandardScaler()  \n",
    "\n",
    "# Fitting only on training data\n",
    "scaler.fit(train_data)  \n",
    "train_data = scaler.transform(train_data)  \n",
    "\n",
    "# Applying same transformation to test data\n",
    "test_data = scaler.transform(test_data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_eval(actual, pred):\n",
    "    \n",
    "    confusion = pd.crosstab(actual, pred, rownames=['Actual'], colnames=['Predicted'])\n",
    "    TP = confusion.loc['>50K','>50K']\n",
    "    TN = confusion.loc['<=50K','<=50K']\n",
    "    FP = confusion.loc['<=50K','>50K']\n",
    "    FN = confusion.loc['>50K','<=50K']\n",
    "\n",
    "    accuracy = ((TP+TN))/(TP+FN+FP+TN)\n",
    "    precision = (TP)/(TP+FP)\n",
    "    recall = (TP)/(TP+FN)\n",
    "    f_measure = (2*recall*precision)/(recall+precision)\n",
    "    sensitivity = TP / (TP + FN)\n",
    "    specificity = TN / (TN + FP)\n",
    "    error_rate = 1 - accuracy\n",
    "    \n",
    "    out = {}\n",
    "    out['accuracy'] =  accuracy\n",
    "    out['precision'] = precision\n",
    "    out['recall'] = recall\n",
    "    out['f_measure'] = f_measure\n",
    "    out['sensitivity'] = sensitivity\n",
    "    out['specificity'] = specificity\n",
    "    out['error_rate'] = error_rate\n",
    "    \n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Model Development\n",
    "### 4.2.1. Decision Tree\n",
    "\n",
    "For the decision tree classifier, I experimented with the splitting criteria, minimum samples required to split, max depth of the tree, minimum samples required at the leaf level and the maximum features to consider when looking for the best split. The following values of the parameters attained the best accuracy during classification. Results in the table below.\n",
    "\n",
    "*\t**Splitting criteria:** Gini Index (Using Gini Index marginally outperformed Entropy with a higher accuracy.)\n",
    "*\t**Min samples required to split:** 5% (Best amongst 1%, 10% and 5%.)\n",
    "*\t**Max Depth:** None\n",
    "*\t**Min samples required at leaf:**  0.1 % (Best amongst 1%, 5% and 0.1%.)\n",
    "*\t**Max features:** number of features (Performs better than 'auto', 'log2' and 'sqrt'.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#printmd('### 3.1.1. Model Development ')\n",
    "\n",
    "# Gini \n",
    "clf_gini = tree.DecisionTreeClassifier(criterion = 'gini', min_samples_split = 0.05, min_samples_leaf = 0.001, max_features = None)\n",
    "clf_gini = clf_gini.fit(train_data, train_label)\n",
    "clf_gini_pred = clf_gini.predict(test_data)\n",
    "DTree_Gini = model_eval(test_label, clf_gini_pred)\n",
    "print('Desicion Tree using Gini Index : %.2f percent.' % (round(DTree_Gini['accuracy']*100,2)))\n",
    "\n",
    "\n",
    "# Entropy\n",
    "clf_entropy = tree.DecisionTreeClassifier(criterion = 'entropy', min_samples_split = 0.05, min_samples_leaf = 0.001)\n",
    "clf_entropy = clf_entropy.fit(train_data, train_label)\n",
    "clf_entropy_pred = clf_entropy.predict(test_data)\n",
    "DTree_Entropy = model_eval(test_label, clf_entropy_pred)\n",
    "print('Desicion Tree using Entropy : %.2f percent.' % (round(DTree_Entropy['accuracy']*100,2)))\n",
    "\n",
    "\n",
    "#printmd('### 3.1.2. Model Evaulation ')\n",
    "ovl_dtree = round(pd.DataFrame([DTree_Entropy, DTree_Gini], index = ['DTree_Entropy','DTree_Gini']),4)\n",
    "display(ovl_dtree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot tree\n",
    "\n",
    "# from sklearn.tree import export_graphviz\n",
    "# from IPython.display import SVG\n",
    "# from graphviz import Source\n",
    "\n",
    "\n",
    "\n",
    "# graph = Source( tree.export_graphviz(clf, out_file=None, feature_names=train_data.columns))\n",
    "\n",
    "# # Display Tree\n",
    "# SVG(graph.pipe(format='svg'))\n",
    "\n",
    "# # Save Tree as PNG\n",
    "# png_bytes = graph.pipe(format='png')\n",
    "# with open('dtree_pipe.png','wb') as f:\n",
    "#     f.write(png_bytes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2. Artificial Neural Network\n",
    "For the ANN classifier, I experimented with the activation function, the solver for weight optimization, regularization term and learning schedule for weight updates. The following values of the parameters attained the best accuracy during classification. Other parameters were neither applicable to the 'adam' solver nor did it improve the performance of the model. Results in the table below.\n",
    "\n",
    "*\t**Activation:** Logistic (Marginally outperformed 'relu', 'tanh' and 'identity' functions.)\n",
    "*   **Solver:** Adam (Works well on relatively large datasets with thousands of training samples or more)\n",
    "*   **Alpha:** 1e-4 (Best amongst 1, 1e-1, 1e-2, 1e-3, 1e-4 and 1e-5)\n",
    "*   **Learning Rate:**  'invscaling' (Gradually decreases the learning rate at each time step 't' using an inverse scaling exponent of 'power_t'.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tan H\n",
    "ann_tanh = MLPClassifier(activation = 'tanh', solver='lbfgs', alpha=1e-1, hidden_layer_sizes=(10, 2), random_state=1, warm_start=True)\n",
    "ann_tanh.fit(train_data, train_label)                         \n",
    "ann_tanh_pred = ann_tanh.predict(test_data)\n",
    "ANN_TanH = model_eval(test_label, ann_tanh_pred)\n",
    "print('ANN using TanH and lbfgs solver : %.2f percent.' % (round(ANN_TanH['accuracy']*100,2)))\n",
    "\n",
    "\n",
    "# Relu\n",
    "ann_relu = MLPClassifier(activation = 'relu', solver='adam', alpha=1e-1, \n",
    "                    hidden_layer_sizes=(5, 2), random_state=1,\n",
    "                    learning_rate  = 'invscaling',\n",
    "                    warm_start = True)\n",
    "ann_relu.fit(train_data, train_label)                         \n",
    "ann_relu_pred = ann_relu.predict(test_data)\n",
    "ANN_relu = model_eval(test_label, ann_relu_pred)\n",
    "print('ANN using relu and adam solver : %.2f percent.' % (round(ANN_relu['accuracy']*100,2)))\n",
    "\n",
    "# Log\n",
    "ann_log = MLPClassifier(activation = 'logistic', solver='adam', \n",
    "                    alpha=1e-4, hidden_layer_sizes=(5, 2),\n",
    "                    learning_rate  = 'invscaling', \n",
    "                    random_state=1, warm_start = True)\n",
    "ann_log.fit(train_data, train_label)                         \n",
    "ann_log_pred = ann_log.predict(test_data)\n",
    "ANN_log = model_eval(test_label, ann_log_pred)\n",
    "print('ANN using logistic and adam solver : %.2f percent.' % (round(ANN_log['accuracy']*100,2)))\n",
    "\n",
    "# Identity\n",
    "ann_identity = MLPClassifier(activation = 'identity', solver='adam', alpha=1e-1, hidden_layer_sizes=(5, 2), random_state=1, warm_start = True)\n",
    "ann_identity.fit(train_data, train_label)                         \n",
    "ann_identity_pred = ann_identity.predict(test_data)\n",
    "ANN_identity = model_eval(test_label, ann_identity_pred)\n",
    "print('ANN using identity and adam solver : %.2f percent.' % (round(ANN_identity['accuracy']*100,2)))\n",
    "\n",
    "#printmd('### 3.2.2. Model Evaulation ')\n",
    "ovl_ann = round(pd.DataFrame([ANN_TanH, ANN_relu, ANN_log, ANN_identity], index = ['ANN_TanH','ANN_relu', 'ANN_log', 'ANN_identity']),4)\n",
    "display(ovl_ann)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.3. Support Vector Machine\n",
    "For the SVM classifier, I experimented with the various available kernels, the penalty of the error term and the tolerance for stopping criteria. The following values of the parameters attained the best accuracy during classification. Results in the table below.\n",
    "\n",
    "*\t**Kernel:** rbf (Marginally outperformed 'linear, 'poly' and 'sigmoid' kernels.)\n",
    "*\t**C, penalty of the error term:** 1 (Best amongst 0.1, 0.5, 1 and 10)\n",
    "*\t**Tolerance for stopping criteria:** 1e-3 (Best amongst 1e-1, 1e-2, 1e-3, 1e-4 and 1e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rbf kernal\n",
    "svm_clf_rbf = svm.SVC(kernel = 'rbf', C = 1, tol = 1e-3)\n",
    "svm_clf_rbf.fit(train_data, train_label)\n",
    "svm_clf_rbf_pred = svm_clf_rbf.predict(test_data)\n",
    "SVM_rbf = model_eval(test_label, svm_clf_rbf_pred)\n",
    "print('SVM using rbf kernel : %.2f percent.' % (round(SVM_rbf['accuracy']*100,2)))\n",
    "\n",
    "# Linear kernel\n",
    "svm_clf_linear = svm.SVC(kernel = 'linear')\n",
    "svm_clf_linear.fit(train_data, train_label)\n",
    "svm_clf_linear_pred = svm_clf_linear.predict(test_data)\n",
    "SVM_linear = model_eval(test_label, svm_clf_linear_pred)\n",
    "print('SVM using linear kernel : %.2f percent.' % (round(SVM_linear['accuracy']*100,2)))\n",
    "\n",
    "\n",
    "# Poly kernal\n",
    "svm_clf_poly = svm.SVC(kernel = 'poly')\n",
    "svm_clf_poly.fit(train_data, train_label)\n",
    "svm_clf_poly_pred = svm_clf_poly.predict(test_data)\n",
    "SVM_poly = model_eval(test_label, svm_clf_poly_pred)\n",
    "print('SVM using poly kernel : %.2f percent.' % (round(SVM_poly['accuracy']*100,2)))\n",
    "\n",
    "\n",
    "svm_clf_sigmoid = svm.SVC(kernel = 'sigmoid')\n",
    "svm_clf_sigmoid.fit(train_data, train_label)\n",
    "svm_clf_sigmoid_pred = svm_clf_sigmoid.predict(test_data)\n",
    "SVM_sigmoid = model_eval(test_label, svm_clf_sigmoid_pred)\n",
    "print('SVM using sigmoid kernel : %.2f percent.' % (round(SVM_sigmoid['accuracy']*100,2)))\n",
    "\n",
    "\n",
    "\n",
    "#printmd('### 3.3.2. Model Evaulation ')\n",
    "ovl_svm = round(pd.DataFrame([SVM_rbf, SVM_linear, SVM_poly, SVM_sigmoid], index = ['SVM_rbf','SVM_linear', 'SVM_poly', 'SVM_sigmoid']),4)\n",
    "display(ovl_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.4. Ensemble Models\n",
    "\n",
    "### 4.2.4.1. Random Forest\n",
    "For the random forests classifier, I experimented with the number of trees, splitting criteria, minimum samples required to split, max depth of the tree, minimum samples required at the leaf level and the maximum features to consider when looking for the best split. The following values of the parameters attained the best accuracy during classification. Results in the table below.\n",
    "\n",
    "*\t**Num estimators:** 100 (Best amongst 10, 50 and 100)\n",
    "*\t**Splitting criteria:** Gini Index (Using Gini Index marginally outperformed Entropy with a higher accuracy.)\n",
    "*\t**Min samples required to split:** 5% (Best amongst 1%, 10% and 5%.)\n",
    "*\t**Max Depth:** None\n",
    "*\t**Min samples required at leaf:**  0.1 % (Best amongst 1%, 5% and 0.1%.)\n",
    "*\t**Max features:** number of features (Performs better than 'auto', 'log2' and 'sqrt'.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gini\n",
    "r_forest_gini = RandomForestClassifier(n_estimators=100, criterion = 'gini', max_features = None,  min_samples_split = 0.05, min_samples_leaf = 0.001)\n",
    "r_forest_gini.fit(train_data, train_label)\n",
    "r_forest_gini_pred = r_forest_gini.predict(test_data)\n",
    "rforest_gini = model_eval(test_label, r_forest_gini_pred)\n",
    "print('Random Forest using Gini Index : %.2f percent.' % (round(rforest_gini['accuracy']*100,2)))\n",
    "\n",
    "# Entropy\n",
    "r_forest_entropy = RandomForestClassifier(n_estimators=100, criterion = 'entropy', max_features = None,  min_samples_split = 0.05, min_samples_leaf = 0.001)\n",
    "r_forest_entropy.fit(train_data, train_label)\n",
    "r_forest_entropy_pred = r_forest_entropy.predict(test_data)\n",
    "rforest_entropy = model_eval(test_label, r_forest_entropy_pred)\n",
    "print('Random Forest using Entropy : %.2f percent.' % (round(rforest_entropy['accuracy']*100,2)))\n",
    "\n",
    "#printmd('### 3.4.1.2. Model Evaulation ')\n",
    "ovl_rf = round(pd.DataFrame([rforest_gini, rforest_entropy], index = ['rforest_gini','rforest_entropy']),4)\n",
    "display(ovl_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.4.2. Adaboost\n",
    "For the adaboost classifier, I experimented with base estimator from which the boosted ensemble is built and number of estimators. The following values of the parameters attained the best accuracy during classification. Results in the table below.\n",
    "\n",
    "*\t**Base Estimator:** DecisionTreeClassifier\n",
    "*\t**Num estimators:** 100 (Best amongst 10, 50 and 100.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada = AdaBoostClassifier(n_estimators=100)                     \n",
    "ada.fit(train_data, train_label)\n",
    "ada_pred = ada.predict(test_data)\n",
    "adaboost = model_eval(test_label, ada_pred)\n",
    "print('Adaboost : %.2f percent.' % (round(adaboost['accuracy']*100,2)))\n",
    "\n",
    "#printmd('### 3.4.2.2. Model Evaulation ')\n",
    "ovl_ada = round(pd.DataFrame([adaboost], index = ['adaboost']),4)\n",
    "display(ovl_ada)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.5. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(penalty = 'l2', dual = False, tol = 1e-4, fit_intercept = True, \n",
    "                            solver = 'liblinear')\n",
    "log_reg.fit(train_data, train_label)\n",
    "log_reg_pred = log_reg.predict(test_data)\n",
    "logistic_reg = model_eval(test_label, log_reg_pred)\n",
    "print('Logistic Regression : %.2f percent.' % (round(logistic_reg['accuracy']*100,3)))\n",
    "\n",
    "#printmd('### 3.5.2. Model Evaulation ')\n",
    "ovl_logreg = round(pd.DataFrame([logistic_reg], index = ['logistic_reg']),4)\n",
    "display(ovl_logreg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.6. k Nearest Neighbours\n",
    "For the K nearest neighbours classifier, I experimented with the num of neighbours values, every odd number ranging from 1 to 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn_outs = []\n",
    "for i in range(1,50,2):\n",
    "    knn = KNeighborsClassifier(n_neighbors=i)\n",
    "    knn.fit(train_data, train_label) \n",
    "    knn_pred = knn.predict(test_data)\n",
    "    knn_perf = model_eval(test_label, knn_pred)\n",
    "    knn_perf['k'] = i\n",
    "    knn_outs.append(knn_perf)\n",
    "\n",
    "ovl_knn = round(pd.DataFrame(knn_outs),4)\n",
    "display(ovl_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 5. Model Evaluation\n",
    "## 5.1. Overall Performance Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_eval = pd.concat([ovl_dtree, ovl_ann, ovl_svm, ovl_rf, ovl_ada, ovl_logreg], axis = 0)\n",
    "overall_eval.sort_values(by = ['f_measure', 'accuracy'], ascending = False, inplace = True)\n",
    "\n",
    "printmd('Combing the performance statistics of all the model developed, as seen in table below, \\\n",
    "        we see that the ensemble model Adaboost hast the highest F-measure (0.6833), precision (0.7812) \\\n",
    "        and accuracy (0.8647). The Artificial neural network models are only marginally being in terms of \\\n",
    "        accuracy and F-measure. Almost all the model have an accuracy greater than 0.84, expect for two SVM \\\n",
    "        models. The table below lists the accuracy, error rate, F-measure, precision, recall, sensitivity and \\\n",
    "        specificity of all the models developed.')\n",
    "\n",
    "display(overall_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateRoc(test_data, test_label, classifiers, pred_labels, plot_labels, limiter):\n",
    "    \n",
    "    color = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:purple', 'tab:brown', 'tab:pink', 'tab:gray', 'tab:olive', 'tab:cyan']\n",
    "    \n",
    "    y_test = label_binarize(test_label, classes=['<=50K', '>50K'])\n",
    "    plt.figure()\n",
    "    \n",
    "    for i in range(len(classifiers)):\n",
    "        \n",
    "        if plot_labels[i] not in limiter:\n",
    "            continue\n",
    "        \n",
    "        y_score = classifiers[i].predict_proba(test_data)\n",
    "        pos_class_index = list(np.unique(pred_labels[i])).index('>50K')\n",
    "        \n",
    "        fpr, tpr, thres = metrics.roc_curve(y_test.ravel(),y_score[:,pos_class_index], pos_label=1)\n",
    "                               \n",
    "        lw = 2\n",
    "        plt.plot(fpr, tpr, color=color[i % len(color)],lw=lw, label=plot_labels[i])\n",
    "        \n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic (ROC)')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "classifier_list = [clf_gini\n",
    "                ,clf_entropy\n",
    "                ,ann_tanh\n",
    "                ,ann_relu\n",
    "                ,ann_log\n",
    "                ,ann_identity\n",
    "#                 ,svm_clf_rbf\n",
    "#                 ,svm_clf_linear\n",
    "#                 ,svm_clf_poly\n",
    "#                 ,svm_clf_sigmoid\n",
    "                ,r_forest_gini\n",
    "                ,r_forest_entropy\n",
    "                ,ada\n",
    "                ,log_reg\n",
    "                ] \n",
    "pred_list = [clf_gini_pred\n",
    "            ,clf_entropy_pred\n",
    "            ,ann_tanh_pred\n",
    "            ,ann_relu_pred\n",
    "            ,ann_log_pred\n",
    "            ,ann_identity_pred\n",
    "#             ,svm_clf_rbf_pred\n",
    "#             ,svm_clf_linear_pred\n",
    "#             ,svm_clf_poly_pred\n",
    "#             ,svm_clf_sigmoid_pred\n",
    "            ,r_forest_gini_pred\n",
    "            ,r_forest_entropy_pred\n",
    "            ,ada_pred\n",
    "            ,log_reg_pred\n",
    "            ]\n",
    "\n",
    "clf_labels = ['DTree Gini'\n",
    "            ,'DTree Entropy'\n",
    "            ,'ANN TanH'\n",
    "            ,'ANN relu'\n",
    "            ,'ANN Logistic'\n",
    "            ,'ANN Identity'\n",
    "#             ,svm_clf_rbf_pred\n",
    "#             ,svm_clf_linear_pred\n",
    "#             ,svm_clf_poly_pred\n",
    "#             ,svm_clf_sigmoid_pred\n",
    "            ,'RForest Gini'\n",
    "            ,'RForest Entropy'\n",
    "            ,'Adaboost'\n",
    "            ,'Logistic Regression'\n",
    "            ]\n",
    "\n",
    "limiter = ['Adaboost', 'ANN TanH', 'ANN relu', 'ANN Logistic', 'Logistic Regression']\n",
    "generateRoc(test_data, test_label, classifier_list, pred_list, clf_labels, limiter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot above of the receiver operating characteristic curve for the top 5 models; Adaboost, ANN with logistics activation function, ANN with relu activation function, ANN with tanH activation function and logistic regression model. I chose to plot only the top 5 models as the ROC curves of most of the models overlap and the it is not easy to interpret the curve. \n",
    "\n",
    "From figure, we can see that the ROC curve of the Adaboost model has the highest lift and is closest to the top left corner (TPR of 1 and FPR of 0) of the plot. The Adaboost model's curve clearly separates itself from the ROC curves of the other 4 models, which overlap with each other. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 5. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I choose **Adaboost** model as my preferred my approach. The Adaboost model not only has the **highest accuracy**, but also has the **highest precision and F-measure** of all the models developed as a part of this analysis. The advantages of using Adaboost over other models is that they are very simple to implement. Since they are made up of weak individual learners, they are less susceptible to overfitting. However, Adaboost is sensitive to noisy data and outliers. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
