{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99c8406a",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'allbp_test.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16248/2055052226.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    471\u001b[0m train_data = pd.read_csv('adult.data',\n\u001b[0;32m    472\u001b[0m                          sep=' ,', names=column_names, encoding='utf-8', engine='python')\n\u001b[1;32m--> 473\u001b[1;33m test_data = pd.read_csv('allbp_test.csv',\n\u001b[0m\u001b[0;32m    474\u001b[0m                         sep=' ,', names=column_names, encoding='utf-8', engine='python')\n\u001b[0;32m    475\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    676\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 575\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    576\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    930\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 932\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    933\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    934\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1214\u001b[0m             \u001b[1;31m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1215\u001b[0m             \u001b[1;31m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1216\u001b[1;33m             self.handles = get_handle(  # type: ignore[call-overload]\n\u001b[0m\u001b[0;32m   1217\u001b[0m                 \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1218\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    784\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    785\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 786\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    787\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'allbp_test.csv'"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    CSCI 4144 Data Mining and Warehousing - Project\n",
    "    Title: Implementing C4.5 Decision Tree Algorithm for Medical Data Mining\n",
    "    Author: Keelin Sekerka-Bajbus B00739421\n",
    "\n",
    "    Filename: c45.py\n",
    "    Program Description:\n",
    "        This program implements Quinlan's C4.5 decision tree from scratch, and conducts experiments using the UCI\n",
    "        ML repo Thyroid disease dataset for binding proteins (allbp). Specifically, this program uses two classes,\n",
    "        Node and C45Tree to construct the decision tree using the Information gain ratio from information theory.\n",
    "\n",
    "    Data Source:\n",
    "    - UCI Machine Learning Repository, Thyroid Disease Data Set https://archive.ics.uci.edu/ml/datasets/thyroid+disease\n",
    "        (Using allbp.data and allbp.test files, 2800 instances in training, 972 testing)\n",
    "\n",
    "    References Consulted:\n",
    "    [1] Data Mining (3rd Edition) Chapter 8 https://doi-org.ezproxy.library.dal.ca/10.1016/B978-0-12-381479-1.00008-3\n",
    "    [2] Pandas library documentation https://pandas.pydata.org/docs/\n",
    "    [3] https://stackoverflow.com/questions/32617811/imputation-of-missing-values-for-categories-in-pandas\n",
    "    [4] collections Counter documentation https://docs.python.org/3/library/collections.html#collections.Counter\n",
    "'''\n",
    "\n",
    "import collections\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, x, y, attribute_list, node_type):\n",
    "        self.data = x\n",
    "        self.labels = y\n",
    "        self.attributes_list = attribute_list\n",
    "        self.best_attribute = None\n",
    "        self.split_criterion = None\n",
    "        self.split_up_down = None\n",
    "        self.node_type = node_type\n",
    "        self.leaf_label = None\n",
    "        self.depth = 0\n",
    "        self.children = []\n",
    "        self.parent = None\n",
    "\n",
    "    def __lt__(self, other):\n",
    "        return self.depth < other.depth\n",
    "\n",
    "    def predict_leaf_class(self):\n",
    "        \"\"\"\n",
    "            Computes the frequency of classes in partition D, output the leaf node label predicted class\n",
    "        :return: pred_class\n",
    "        \"\"\"\n",
    "        # takes frequency of classes in D to determine the majority class to set as output leaf label\n",
    "        freq_classes = collections.Counter(self.labels)  # [4]\n",
    "        pred_class = max(freq_classes, key=freq_classes.get)\n",
    "        self.leaf_label = pred_class\n",
    "        return pred_class\n",
    "\n",
    "    def print_node(self):\n",
    "        \"\"\"\n",
    "            Print node values\n",
    "        \"\"\"\n",
    "        print('best att-', self.best_attribute, 'split_crit-', self.split_up_down, self.split_criterion, 'type-',\n",
    "              self.node_type, 'depth-',\n",
    "              self.depth, 'class label-', self.leaf_label)\n",
    "\n",
    "    def copy(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class C45Tree:\n",
    "    def __init__(self, attributes, data):\n",
    "        self.tree_nodes = []\n",
    "        self.depth = 0\n",
    "        self.num_leaves = 0\n",
    "        self.root_node = None\n",
    "        self.attributes = attributes[:-1]\n",
    "        self.dataset = data\n",
    "\n",
    "    def train(self, x_train, y_train):\n",
    "        \"\"\"\n",
    "            Helper function to grow tree recursively, creates root node for the tree and initializes the recursion for\n",
    "            training the tree.\n",
    "        :param x_train:\n",
    "        :param y_train:\n",
    "        \"\"\"\n",
    "        # create root node, put data partition in node\n",
    "        self.root_node = Node(x_train, y_train, self.attributes, 'root')\n",
    "        self.tree_nodes.append(self.root_node)\n",
    "        # call grow_tree with root node as base\n",
    "        self.grow_tree(self.root_node, self.attributes, (x_train, y_train))\n",
    "\n",
    "    def grow_tree(self, prev_node, attribute_list, D):\n",
    "        \"\"\"\n",
    "            Uses C4.5 decision tree algorithm to grow a tree during training, based on pseudocode from [1].\n",
    "        :param attribute_list:\n",
    "        :param D:\n",
    "        :param prev_node:\n",
    "        :return: N, the new node\n",
    "        \"\"\"\n",
    "        if prev_node is not None and prev_node.parent is not None:\n",
    "            if prev_node not in prev_node.parent.children:\n",
    "                prev_node.parent.children.append(prev_node)\n",
    "\n",
    "        # check for termination cases\n",
    "        # check if all tuples in D are in the same class\n",
    "        if self.check_same_class_labels(D[1]):\n",
    "            N = Node(D[0], D[1], attribute_list, 'leaf')\n",
    "            N.depth = prev_node.depth + 1\n",
    "            N.predict_leaf_class()  # determine the class of the leaf\n",
    "            N.best_attribute = str(prev_node.best_attribute)\n",
    "            N.split_up_down = prev_node.split_up_down\n",
    "            N.split_criterion = prev_node.split_criterion\n",
    "            self.tree_nodes.append(N)\n",
    "            prev_node.children.append(N)\n",
    "            N.parent = prev_node\n",
    "            return N\n",
    "\n",
    "        # check if attribute list is empty, do majority voting on class\n",
    "        if not attribute_list:\n",
    "            N = Node(D[0], D[1], attribute_list, 'leaf')\n",
    "            N.depth = prev_node.depth + 1\n",
    "            N.predict_leaf_class()  # determine the class of the leaf\n",
    "            N.best_attribute = str(prev_node.best_attribute)\n",
    "            N.split_criterion = prev_node.split_criterion\n",
    "            N.split_up_down = prev_node.split_up_down\n",
    "            self.tree_nodes.append(N)\n",
    "            prev_node.children.append(N)\n",
    "            N.parent = prev_node\n",
    "            return N\n",
    "\n",
    "        # create new node\n",
    "        N = Node(D[0], D[1], attribute_list, 'node')\n",
    "        N.depth = prev_node.depth + 1\n",
    "        N.parent = prev_node\n",
    "        # conduct attribute selection method, label node with the criterion\n",
    "        best_attribute, crit_split_val = self.attribute_selection_method(D, attribute_list)\n",
    "\n",
    "        N.best_attribute = best_attribute  # label node with best attribute\n",
    "        N.split_criterion = crit_split_val  # for discrete\n",
    "        if best_attribute == '':\n",
    "            # early stop\n",
    "            N.best_attribute = str(best_attribute)\n",
    "            N.split_up_down = None\n",
    "            N.node_type = 'leaf'\n",
    "            N.data = prev_node.data\n",
    "            N.labels = prev_node.labels\n",
    "            N.predict_leaf_class()\n",
    "            self.tree_nodes.append(N)\n",
    "            prev_node.children.append(N)\n",
    "            return N\n",
    "\n",
    "        # remove split attribute from attribute list\n",
    "        if best_attribute in attribute_list:\n",
    "            attribute_list.remove(best_attribute)\n",
    "\n",
    "        # check if attribute is discrete NOTE THIS LINE NEEDS TO BE MODIFIED FOR DIFFERENT DATASET\n",
    "        if len(self.dataset[\n",
    "                   best_attribute].unique()) > 5:  # max 5 discrete categories in attributes from Thyroid set\n",
    "            # continuous, divide up data at mid point of the values ai + ai1/2\n",
    "            l_part, r_part, split_val = self.continuous_attribute_data_partition(D, best_attribute)\n",
    "            N.split_criterion = split_val\n",
    "            N.split_up_down = 'UP'\n",
    "            l_child = self.grow_tree(N, attribute_list, l_part)  # upper -> att_val > split_val\n",
    "            N_V = Node(D[0], D[1], attribute_list, 'node')\n",
    "            N_V.depth = N.depth\n",
    "            N_V.best_attribute = best_attribute\n",
    "            N_V.split_criterion = split_val\n",
    "            N_V.parent = prev_node\n",
    "            N_V.split_up_down = 'DOWN'\n",
    "            r_child = self.grow_tree(N_V, attribute_list, r_part)  # lower -> att_val <= split_val\n",
    "            N.children.append(l_child)\n",
    "            N_V.children.append(r_child)\n",
    "            N.parent = prev_node\n",
    "            self.tree_nodes.append(N)\n",
    "            self.tree_nodes.append(N_V)\n",
    "            prev_node.children.append(N)\n",
    "            prev_node.children.append(N_V)\n",
    "            return N\n",
    "        else:\n",
    "            # discrete, partition based on unique values of attribute to create nodes for recursion\n",
    "            vals = self.dataset[best_attribute].unique()  # D[0][best_attribute].unique()\n",
    "            for v in list(vals):\n",
    "                data_part = self.partition_data(D, best_attribute, v)\n",
    "\n",
    "                if not data_part:  # TOGGLED TO EMPTY CAUSES 2 LEAVES ONLY TO BE MADE ** check this\n",
    "                    # majority class leaf node computed of D\n",
    "                    L = Node(D[0], D[1], attribute_list, 'leaf')\n",
    "                    L.depth = N.depth + 1\n",
    "                    L.best_attribute = best_attribute\n",
    "                    L.split_criterion = v\n",
    "                    L.predict_leaf_class()  # determine the class of the leaf\n",
    "                    self.tree_nodes.append(L)\n",
    "                    N.children.append(L)\n",
    "                    L.parent = N\n",
    "                else:\n",
    "                    # recursion\n",
    "                    N_V = Node(D[0], D[1], attribute_list, 'node')\n",
    "                    N_V.depth = N.depth\n",
    "                    N_V.best_attribute = best_attribute\n",
    "                    N_V.split_criterion = v\n",
    "                    N_V.parent = prev_node\n",
    "                    N_V.parent.children.append(N_V)\n",
    "                    child = self.grow_tree(N_V, attribute_list, data_part)\n",
    "\n",
    "        if N not in self.tree_nodes:\n",
    "            self.tree_nodes.append(N)\n",
    "            prev_node.children.append(N)\n",
    "        return N\n",
    "\n",
    "    def continuous_attribute_data_partition(self, D, attribute):\n",
    "        \"\"\"\n",
    "            Creates data partitions (left and right) for continuous attributes, computing the mid point that\n",
    "            enables the best information gain ratio to be calculated from the partition.\n",
    "        :param D:\n",
    "        :param attribute:\n",
    "        :return: l_part, r_part, split_val\n",
    "        \"\"\"\n",
    "        # sort the data, find the value that will gain the max info gain ratio\n",
    "        data = D[0].sort_values(by=[attribute])\n",
    "        split_val = 0\n",
    "        best_igr = 0\n",
    "        l_part = []\n",
    "        r_part = []\n",
    "\n",
    "        for i in range(0, len(data) - 1):\n",
    "            mid_point = (float(data.iloc[i][attribute]) + float(data.iloc[i + 1][attribute])) / 2\n",
    "            left_d = D[0].loc[pd.to_numeric(D[0][attribute]) > mid_point]\n",
    "            left_idx = D[0].index[pd.to_numeric(D[0][attribute]) > mid_point]\n",
    "            left_y = D[1].loc[left_idx]\n",
    "            right_d = D[0].loc[pd.to_numeric(D[0][attribute]) <= mid_point]\n",
    "            right_idx = D[0].index[pd.to_numeric(D[0][attribute]) <= mid_point]\n",
    "            right_y = D[1].loc[right_idx]\n",
    "            igr = self.compute_info_gain_ratio_continuous(D, left_y, right_y)\n",
    "\n",
    "            if igr >= best_igr:\n",
    "                best_igr = igr\n",
    "                split_val = mid_point\n",
    "                l_part = (left_d, left_y)\n",
    "                r_part = (right_d, right_y)\n",
    "\n",
    "        return l_part, r_part, split_val\n",
    "\n",
    "    def compute_info_gain_ratio_continuous(self, D, left_y, right_y):\n",
    "        \"\"\"\n",
    "            Computes the information gain ratio for a continuous attribute partition\n",
    "        :return info_gain_ratio\n",
    "        \"\"\"\n",
    "        l_y = left_y\n",
    "        r_y = right_y\n",
    "\n",
    "        dataset_entropy = self.data_entropy(D[1])\n",
    "        l_part_entropy = self.data_entropy(l_y)\n",
    "        l_p_j = float(len(l_y) / len(D))\n",
    "        l_ent = l_p_j * l_part_entropy\n",
    "        r_part_entropy = self.data_entropy(r_y)\n",
    "        r_p_j = float(len(r_y) / len(D))\n",
    "        r_ent = r_p_j * r_part_entropy\n",
    "\n",
    "        split_info = - self.split_info(l_p_j) - self.split_info(r_p_j)\n",
    "        att_ent = l_ent + r_ent\n",
    "\n",
    "        if split_info == 0:  # prevent division by zero for ratio\n",
    "            return 0\n",
    "        else:\n",
    "            info_gain = self.information_gain(dataset_entropy, att_ent)\n",
    "            info_gain_ratio = self.information_gain_ratio(info_gain,\n",
    "                                                          split_info)\n",
    "        return info_gain_ratio\n",
    "\n",
    "    @staticmethod\n",
    "    def check_same_class_labels(labels):\n",
    "        \"\"\"\n",
    "            Checks set of labels to ensure they are of the same class type\n",
    "        :param labels:\n",
    "        :return: bool\n",
    "        \"\"\"\n",
    "        if len(set(labels)) == 1:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def attribute_selection_method(self, D, attribute_list):\n",
    "        \"\"\"\n",
    "            Attribute Selection Method for decision tree as discussed in [1] (Figure 8.3), selects attribute that\n",
    "            provides the best information gain ratio as a result.\n",
    "        :param D:\n",
    "        :param attribute_list:\n",
    "        :return: best_attribute\n",
    "        \"\"\"\n",
    "        best_attribute = ''\n",
    "        dataset_entropy = self.data_entropy(D[1])\n",
    "        best_info_gain_ratio = 0.0\n",
    "        split_val = ''\n",
    "\n",
    "        for attribute in attribute_list:\n",
    "            # a_idx = self.attributes.get(attribute) MIGHT NEED THIS\n",
    "            v = D[0][attribute].unique()  # find v distinct values of attribute\n",
    "            att_ent = 0.0\n",
    "            split_info = 0.0\n",
    "            curr_val = ''\n",
    "            val_ent = 0.0\n",
    "            for val in v:\n",
    "                data_partition = self.partition_data(D, attribute, val)\n",
    "                partition_labels = data_partition[1]\n",
    "                part_entropy = self.data_entropy(partition_labels)\n",
    "                p_j = float(len(data_partition[1]) / len(D[1]))\n",
    "                att_ent = att_ent + (p_j * part_entropy)\n",
    "                split_info = split_info - self.split_info(p_j)\n",
    "\n",
    "                if part_entropy > val_ent:\n",
    "                    val_ent = part_entropy\n",
    "                    curr_val = val\n",
    "\n",
    "            # Best Attribute checks\n",
    "            if split_info == 0:  # prevent division by zero for ratio\n",
    "                continue\n",
    "            else:\n",
    "                info_gain = self.information_gain(dataset_entropy, att_ent)\n",
    "                info_gain_ratio = self.information_gain_ratio(info_gain,\n",
    "                                                              split_info)  # calculate info gain ratio to select\n",
    "\n",
    "            # compare the top performing attribute info gain ratio\n",
    "            if info_gain_ratio > best_info_gain_ratio:\n",
    "                best_info_gain_ratio = info_gain_ratio\n",
    "                best_attribute = attribute\n",
    "                split_val = curr_val\n",
    "        return best_attribute, split_val\n",
    "\n",
    "    def class_prob(self, feature_label, labels):\n",
    "        \"\"\"\n",
    "            Computes class probabilities from labels\n",
    "        :param feature_label:\n",
    "        :param labels:\n",
    "        :return: p\n",
    "        \"\"\"\n",
    "        c = collections.Counter(labels)  # [4]\n",
    "        p = c[feature_label] / len(labels)\n",
    "        return float(p)\n",
    "\n",
    "    def data_entropy(self, labels):\n",
    "        \"\"\"\n",
    "            Computes the Entropy, or Info(D) [1]\n",
    "        :param labels:\n",
    "        :return: entropy\n",
    "        \"\"\"\n",
    "        entropy = 0.0\n",
    "        class_freq = collections.Counter(labels)  # [4]\n",
    "        for l in class_freq.keys():\n",
    "            p = float(class_freq[l] / len(labels))\n",
    "            entropy = entropy - math.log(p, 2)\n",
    "        return entropy\n",
    "\n",
    "    def information_gain(self, dataset_entropy, attribute_entropy):\n",
    "        \"\"\"\n",
    "            Computes information gain based on the data entropy and attribute entropy [1]\n",
    "        :param dataset_entropy:\n",
    "        :param attribute_entropy:\n",
    "        :return: gain\n",
    "        \"\"\"\n",
    "        gain = dataset_entropy - attribute_entropy\n",
    "        return gain\n",
    "\n",
    "    def split_info(self, p_j):\n",
    "        \"\"\"\n",
    "            Computes the information split, used in gain ratio [1]\n",
    "        :param p_j:\n",
    "        :return: info_split\n",
    "        \"\"\"\n",
    "        # error protection for zero case\n",
    "        if p_j == 0:\n",
    "            return 0\n",
    "\n",
    "        info_split = (p_j * math.log(p_j, 2))\n",
    "        return info_split\n",
    "\n",
    "    def information_gain_ratio(self, gain, split_info):\n",
    "        \"\"\"\n",
    "            Computes information gain ratio [1]\n",
    "        :param gain:\n",
    "        :param split_info:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        gain_ratio = float(gain / split_info)\n",
    "        return gain_ratio\n",
    "\n",
    "    def partition_data(self, D, attribute, val):\n",
    "        \"\"\"\n",
    "            Partitions a dataset D based on the value of a specific attribute\n",
    "        :param D:\n",
    "        :param attribute:\n",
    "        :param val:\n",
    "        :return: part, part_y\n",
    "        \"\"\"\n",
    "        part = D[0].loc[D[0][attribute] == val]\n",
    "        part_idx = D[0].index[D[0][attribute] == val]\n",
    "        part_y = D[1].loc[part_idx]\n",
    "        return part, part_y\n",
    "\n",
    "    def test_tree(self, test_sample, node):\n",
    "        \"\"\"\n",
    "            Using recursion, we go through each node (from the root through to the children) to find a leaf label\n",
    "            to classify the test sample as a prediction.\n",
    "        :param test_sample:\n",
    "        :param node:\n",
    "        :return: node.leaf_label, or recursion\n",
    "        \"\"\"\n",
    "\n",
    "        if node.node_type == 'leaf':\n",
    "            return node.leaf_label\n",
    "        else:\n",
    "            for child in node.children:\n",
    "                if (child.best_attribute is None or child.best_attribute == '') and child.node_type == 'leaf':\n",
    "                    return self.test_tree(test_sample, child)\n",
    "\n",
    "                if (child.best_attribute is None or child.best_attribute == '') and child.node_type == 'node':\n",
    "                    pass\n",
    "                else:\n",
    "                    if child.split_criterion == test_sample[child.best_attribute]:\n",
    "                        return self.test_tree(test_sample, child)\n",
    "                    else:\n",
    "                        if child.split_up_down == 'UP':\n",
    "                            # check if att_val > split_criterion\n",
    "                            if pd.to_numeric(test_sample[child.best_attribute]) > float(child.split_criterion):\n",
    "                                return self.test_tree(test_sample, child)\n",
    "                            else:\n",
    "                                pass\n",
    "                        elif child.split_up_down == 'DOWN':\n",
    "                            if pd.to_numeric(test_sample[child.best_attribute]) <= float(child.split_criterion):\n",
    "                                return self.test_tree(test_sample, child)\n",
    "                            else:\n",
    "                                pass\n",
    "\n",
    "    def predict(self, test_x, test_y):  # TODO Add this functionality from the code in main routine\n",
    "        # uses test set to predict class labels from the constructed tree\n",
    "        preds = []\n",
    "        true_pred = 0\n",
    "        for i in range(len(test_x)):\n",
    "            tester_instance = test_x.iloc[i]\n",
    "            pred = self.test_tree(tester_instance, self.root_node)\n",
    "            # print(str(i), 'pred', pred, 'label', y.iloc[i])\n",
    "            if pred == test_y.iloc[i]:\n",
    "                true_pred += 1\n",
    "            preds.append(pred)\n",
    "\n",
    "        return true_pred, preds\n",
    "\n",
    "    def print_tree(self):\n",
    "        nodes_created = sorted(self.tree_nodes)\n",
    "        for n in nodes_created:\n",
    "            n.print_node()\n",
    "            for d in n.children:\n",
    "                d.print_node()\n",
    "            print()\n",
    "        return\n",
    "\n",
    "\n",
    "# Main experiment routine, read dataset, dropna values using pandas, split x and y matrices to pass in to tree\n",
    "# make test and training splits THYROID dataset\n",
    "# declare tree, initialize root node / start training and growing the tree\n",
    "# print the tree and stats\n",
    "# conduct testing with test set for predictions, analyze results (accuracy, recall, etc)\n",
    "\n",
    "# DATA LOADING AND PRE-PROCESSING TEST AND TRAINING DATA\n",
    "column_names = ['age', 'sex', 'on thyroxine', 'query on thyroxine', 'on antithyroid medication', 'sick', 'pregnant',\n",
    "                'thyroid surgery', 'I131 treatment', 'query hypothyroid', 'query hyperthyroid', 'lithium', 'goitre',\n",
    "                'tumor', 'hypopituitary', 'psych', 'TSH measured', 'TSH', 'T3 measured', 'T3', 'TT4 measured', 'TT4',\n",
    "                'T4U measured', 'T4U', 'FTI measured', 'FTI', 'TBG measured', 'TBG', 'referral source', 'Class'\n",
    "                ]\n",
    "\n",
    "train_data = pd.read_csv('adult.data',\n",
    "                         sep=' ,', names=column_names, encoding='utf-8', engine='python')\n",
    "test_data = pd.read_csv('adult.test',\n",
    "                        sep=' ,', names=column_names, encoding='utf-8', engine='python')\n",
    "\n",
    "train_data[['index_dup', 'age']] = train_data['age'].str.split(',', n=1, expand=True)\n",
    "train_data = train_data.drop('index_dup', 1)\n",
    "train_data = train_data.replace('?', pd.NA)\n",
    "# replace ? with most common value\n",
    "train_data = train_data.fillna(train_data.mode().iloc[0])  # [3]\n",
    "\n",
    "test_data[['index_dup', 'age']] = test_data['age'].str.split(',', n=1, expand=True)\n",
    "test_data = test_data.drop('index_dup', 1)\n",
    "test_data = test_data.replace('?', pd.NA)\n",
    "# replace ? with most common value\n",
    "test_data = test_data.fillna(train_data.mode().iloc[0])  # [3]\n",
    "\n",
    "np.random.seed(42)  # replicate results using random seed\n",
    "train_data = sklearn.utils.shuffle(train_data)\n",
    "test_data = sklearn.utils.shuffle(test_data)\n",
    "\n",
    "x_train = train_data.iloc[:, :-1]\n",
    "y_train = train_data.iloc[:, -1]\n",
    "x_test = test_data.iloc[:, :-1]\n",
    "y_test = test_data.iloc[:, -1]\n",
    "\n",
    "y_train = y_train.replace('negative.', 'negative')\n",
    "y_train = y_train.replace('increased  binding  protein.', 'increased  binding  protein')\n",
    "y_train = y_train.replace('decreased  binding  protein.', 'decreased  binding  protein')\n",
    "y_test = y_test.replace('negative.', 'negative')\n",
    "y_test = y_test.replace('increased  binding  protein.', 'increased  binding  protein')\n",
    "y_test = y_test.replace('decreased  binding  protein.', 'decreased  binding  protein')\n",
    "\n",
    "# check class distribution\n",
    "print(y_train.value_counts())\n",
    "print(y_test.value_counts())\n",
    "\n",
    "# 100 sample decision tree\n",
    "system_test = C45Tree(column_names, train_data)\n",
    "\n",
    "f_out = open('initial_testing_results.txt', 'w')\n",
    "\n",
    "# small sample of data\n",
    "f_out.write('First Test: 100 Samples training, 24 Test Samples\\n')\n",
    "x = x_train[:100]\n",
    "y = y_train[:100]\n",
    "testing_x = x_train[101:125]\n",
    "testing_y = y_train[101:125]\n",
    "\n",
    "print('system_test:')\n",
    "system_test.train(x, y)\n",
    "f_out.write('Number of Nodes for 100 sample tree:' + str(len(system_test.tree_nodes)) + '\\n')\n",
    "nodes_created = system_test.tree_nodes\n",
    "\n",
    "leaf_count = 0\n",
    "for n in nodes_created:\n",
    "    # print(n.print_node())\n",
    "    if n.node_type == 'leaf':\n",
    "        leaf_count += 1\n",
    "print('leaves', leaf_count)\n",
    "print(len(system_test.tree_nodes))\n",
    "f_out.write('Number of leaves:' + str(leaf_count) + '\\n')\n",
    "\n",
    "true_pred, preds = system_test.predict(x, y)\n",
    "print('train accuracy:', true_pred / len(x))\n",
    "f_out.write('Training Accuracy:' + str(true_pred / len(x)))\n",
    "true_pred, preds = system_test.predict(testing_x, testing_y)\n",
    "print('test accuracy:', true_pred / len(testing_x))\n",
    "f_out.write('\\tTest Accuracy:' + str(true_pred / len(testing_x)) + '\\n')\n",
    "\n",
    "f_out.write('\\nFirst Test: 500 Samples training, 124 Test Samples\\n')\n",
    "print('500 sample tests:')\n",
    "x_500 = x_train[:500]\n",
    "y_500 = y_train[:500]\n",
    "testing_x = x_train[501:625]\n",
    "testing_y = y_train[501:625]\n",
    "system_test500 = C45Tree(column_names, train_data)\n",
    "system_test500.train(x_500, y_500)\n",
    "print('system500 nodes:', len(system_test500.tree_nodes))\n",
    "\n",
    "leaf_count = 0\n",
    "for n in system_test500.tree_nodes:\n",
    "    # print(n.print_node())\n",
    "    if n.node_type == 'leaf':\n",
    "        leaf_count += 1\n",
    "print('leaves', leaf_count)\n",
    "\n",
    "f_out.write('Number of nodes:' + str(len(system_test500.tree_nodes)) + '\\n')\n",
    "f_out.write('\\t Number of leaves:' + str(leaf_count) + '\\n')\n",
    "true_pred, preds = system_test500.predict(x_500, y_500)\n",
    "print('train accuracy:', true_pred / len(x_500))\n",
    "f_out.write('Train Accuracy:' + str(true_pred / len(x_500)))\n",
    "true_pred, preds = system_test500.predict(testing_x, testing_y)\n",
    "print('test accuracy:', true_pred / len(testing_x))  # RANDOM SEED 42, train acc=0.956 , test acc= 0.9677 55 nodes\n",
    "f_out.write('\\t Test Accuracy:' + str(true_pred / len(testing_x)))\n",
    "leaf_count = 0\n",
    "\n",
    "print('FULL SET')\n",
    "f_out.write('\\nFull Training Data Decision Tree (2800 samples)\\n')\n",
    "true_pred = 0\n",
    "full_system = C45Tree(column_names, train_data)\n",
    "full_system.train(x_train, y_train)\n",
    "print(len(full_system.tree_nodes))\n",
    "leaf_count = 0\n",
    "for n in sorted(full_system.tree_nodes):\n",
    "    # print(n.print_node())\n",
    "    if n.node_type == 'leaf':\n",
    "        leaf_count += 1\n",
    "\n",
    "f_out.write('Number of nodes:' + str(len(full_system.tree_nodes)) + '\\n')\n",
    "f_out.write('Number of leaves:' + str(leaf_count))\n",
    "true_pred, preds = full_system.predict(x_train, y_train)\n",
    "print('Full set train accuracy:', true_pred / len(x_train))\n",
    "f_out.write('\\nFull set train accuracy:' + str(true_pred / len(x_train)))\n",
    "true_pred, preds = full_system.predict(x_test, y_test)\n",
    "f_out.write(\"\\tFull set test accuracy:\" + str(true_pred / len(x_test)))\n",
    "\n",
    "f_out.close()\n",
    "\n",
    "print()\n",
    "full_system.print_tree()\n",
    "\n",
    "print('FURTHER FULL ALLPB DATASET EXPERIMENTS')\n",
    "res_out = open('full_experiments_allpb.txt', 'w')\n",
    "res_out.write('FULL EXPERIMENTATION WITH ALLPB DATASET\\n')\n",
    "res_out.write('Experiment #1 - random state = 24 \\n')\n",
    "\n",
    "np.random.seed(24)  # replicate results using random seed\n",
    "train_data = sklearn.utils.shuffle(train_data, random_state=24)\n",
    "test_data = sklearn.utils.shuffle(test_data, random_state=24)\n",
    "x_train = train_data.iloc[:, :-1]\n",
    "y_train = train_data.iloc[:, -1]\n",
    "x_test = test_data.iloc[:, :-1]\n",
    "y_test = test_data.iloc[:, -1]\n",
    "\n",
    "y_train = y_train.replace('negative.', 'negative')\n",
    "y_train = y_train.replace('increased  binding  protein.', 'increased  binding  protein')\n",
    "y_train = y_train.replace('decreased  binding  protein.', 'decreased  binding  protein')\n",
    "y_test = y_test.replace('negative.', 'negative')\n",
    "y_test = y_test.replace('increased  binding  protein.', 'increased  binding  protein')\n",
    "y_test = y_test.replace('decreased  binding  protein.', 'decreased  binding  protein')\n",
    "\n",
    "exp1C45 = C45Tree(column_names, train_data)\n",
    "exp1C45.train(x_train, y_train)\n",
    "true_pred, preds = exp1C45.predict(x_train, y_train)\n",
    "print('Full set train accuracy:', true_pred / len(x_train))\n",
    "res_out.write('Train Accuracy:' + str(true_pred / len(x_train)))\n",
    "true_pred, preds = exp1C45.predict(x_test, y_test)\n",
    "print('Full set test accuracy:', true_pred / len(x_test))\n",
    "res_out.write('\\tTest Accuracy:' + str(true_pred / len(x_test)))\n",
    "\n",
    "res_out.write('\\nExperiment #2 - random state = 55 \\n')\n",
    "np.random.seed(55)  # replicate results using random seed\n",
    "train_data = sklearn.utils.shuffle(train_data, random_state=55)\n",
    "test_data = sklearn.utils.shuffle(test_data, random_state=55)\n",
    "\n",
    "x_train = train_data.iloc[:, :-1]\n",
    "y_train = train_data.iloc[:, -1]\n",
    "x_test = test_data.iloc[:, :-1]\n",
    "y_test = test_data.iloc[:, -1]\n",
    "\n",
    "y_train = y_train.replace('negative.', 'negative')\n",
    "y_train = y_train.replace('increased  binding  protein.', 'increased  binding  protein')\n",
    "y_train = y_train.replace('decreased  binding  protein.', 'decreased  binding  protein')\n",
    "y_test = y_test.replace('negative.', 'negative')\n",
    "y_test = y_test.replace('increased  binding  protein.', 'increased  binding  protein')\n",
    "y_test = y_test.replace('decreased  binding  protein.', 'decreased  binding  protein')\n",
    "\n",
    "exp2C45 = C45Tree(column_names, train_data)\n",
    "exp2C45.train(x_train, y_train)\n",
    "true_pred, preds = exp2C45.predict(x_train, y_train)\n",
    "print('Full set train accuracy:', true_pred / len(x_train))\n",
    "res_out.write('Train Accuracy:' + str(true_pred / len(x_train)))\n",
    "true_pred, preds = exp2C45.predict(x_test, y_test)\n",
    "print('Full set test accuracy:', true_pred / len(x_test))\n",
    "res_out.write('\\tTest Accuracy:' + str(true_pred / len(x_test)))\n",
    "\n",
    "res_out.write('\\nExperiment #3 - random state = 75 \\n')\n",
    "np.random.seed(75)  # replicate results using random seed\n",
    "train_data = sklearn.utils.shuffle(train_data, random_state=75)\n",
    "test_data = sklearn.utils.shuffle(test_data, random_state=75)\n",
    "\n",
    "x_train = train_data.iloc[:, :-1]\n",
    "y_train = train_data.iloc[:, -1]\n",
    "x_test = test_data.iloc[:, :-1]\n",
    "y_test = test_data.iloc[:, -1]\n",
    "\n",
    "y_train = y_train.replace('negative.', 'negative')\n",
    "y_train = y_train.replace('increased  binding  protein.', 'increased  binding  protein')\n",
    "y_train = y_train.replace('decreased  binding  protein.', 'decreased  binding  protein')\n",
    "y_test = y_test.replace('negative.', 'negative')\n",
    "y_test = y_test.replace('increased  binding  protein.', 'increased  binding  protein')\n",
    "y_test = y_test.replace('decreased  binding  protein.', 'decreased  binding  protein')\n",
    "\n",
    "exp3C45 = C45Tree(column_names, train_data)\n",
    "exp3C45.train(x_train, y_train)\n",
    "true_pred, preds = exp3C45.predict(x_train, y_train)\n",
    "print('Full set train accuracy:', true_pred / len(x_train))\n",
    "res_out.write('Train Accuracy:' + str(true_pred / len(x_train)))\n",
    "true_pred, preds = exp3C45.predict(x_test, y_test)\n",
    "print('Full set test accuracy:', true_pred / len(x_test))\n",
    "res_out.write('\\tTest Accuracy:' + str(true_pred / len(x_test)))\n",
    "res_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f345730e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
